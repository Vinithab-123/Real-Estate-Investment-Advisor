{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8S1ct0ig-MK"
      },
      "source": [
        "Vinitha Buchakkagari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWjRp4d8hD_5"
      },
      "source": [
        "Email: vinithab219@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxCHKXcghNia"
      },
      "source": [
        "Github: https://github.com/Vinithab-123/Real-Estate-Investment-Advisor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71rBsDh6hRTp"
      },
      "source": [
        "**Title: Real Estate Investment Advisor: Predicting Property Profitability & Future Value!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJoHDAazhbQV"
      },
      "source": [
        "**1. Project Setup and Data Loading**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CWsGaFFhh5R"
      },
      "source": [
        "The first step is to import the necessary libraries and load your dataset, india_housing_prices.csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27F2HndFhwE4"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2LqoYNWiCOj",
        "outputId": "1bc1c4db-0a1c-4509-d315-3b631805454e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.7.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-skinny==3.7.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.7.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.7.0 (from mlflow)\n",
            "  Downloading mlflow_tracing-3.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting Flask-CORS<7 (from mlflow)\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huey<3,>=2.5.0 (from mlflow)\n",
            "  Downloading huey-2.5.5-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.44)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.7.0->mlflow)\n",
            "  Downloading databricks_sdk-0.74.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (0.118.3)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (0.5.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (0.38.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.4)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.7.0->mlflow) (0.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.7.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.7.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.7.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.7.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.7.0->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.7.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (2025.11.12)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.7.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.7.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.7.0->mlflow) (4.12.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.7.0-py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.7.0-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huey-2.5.5-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.74.0-py3-none-any.whl (764 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.2/764.2 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: huey, gunicorn, graphql-core, graphql-relay, docker, graphene, Flask-CORS, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
            "Successfully installed Flask-CORS-6.0.1 databricks-sdk-0.74.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 huey-2.5.5 mlflow-3.7.0 mlflow-skinny-3.7.0 mlflow-tracing-3.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPXUO-bYhQwo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier, XGBRegressor # XGBoost is recommended in the plan\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score # For Regression\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score # For Classification\n",
        "import mlflow # For experiment tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuOff-q3iJfa"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLNObfnJhymF",
        "outputId": "429a6456-02e8-42a7-df32-1ad257040421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ID        State      City      Locality      Property_Type  BHK  \\\n",
            "0   1   Tamil Nadu   Chennai   Locality_84          Apartment    1   \n",
            "1   2  Maharashtra      Pune  Locality_490  Independent House    3   \n",
            "2   3       Punjab  Ludhiana  Locality_167          Apartment    2   \n",
            "3   4    Rajasthan   Jodhpur  Locality_393  Independent House    2   \n",
            "4   5    Rajasthan    Jaipur  Locality_466              Villa    4   \n",
            "\n",
            "   Size_in_SqFt  Price_in_Lakhs  Price_per_SqFt  Year_Built  ...  \\\n",
            "0          4740          489.76            0.10        1990  ...   \n",
            "1          2364          195.52            0.08        2008  ...   \n",
            "2          3642          183.79            0.05        1997  ...   \n",
            "3          2741          300.29            0.11        1991  ...   \n",
            "4          4823          182.90            0.04        2002  ...   \n",
            "\n",
            "  Age_of_Property  Nearby_Schools  Nearby_Hospitals  \\\n",
            "0              35              10                 3   \n",
            "1              17               8                 1   \n",
            "2              28               9                 8   \n",
            "3              34               5                 7   \n",
            "4              23               4                 9   \n",
            "\n",
            "   Public_Transport_Accessibility  Parking_Space  Security  \\\n",
            "0                            High             No        No   \n",
            "1                             Low             No       Yes   \n",
            "2                             Low            Yes        No   \n",
            "3                            High            Yes       Yes   \n",
            "4                             Low             No       Yes   \n",
            "\n",
            "                                  Amenities Facing Owner_Type  \\\n",
            "0  Playground, Gym, Garden, Pool, Clubhouse   West      Owner   \n",
            "1  Playground, Clubhouse, Pool, Gym, Garden  North    Builder   \n",
            "2          Clubhouse, Pool, Playground, Gym  South     Broker   \n",
            "3  Playground, Clubhouse, Gym, Pool, Garden  North    Builder   \n",
            "4  Playground, Garden, Gym, Pool, Clubhouse   East    Builder   \n",
            "\n",
            "  Availability_Status  \n",
            "0       Ready_to_Move  \n",
            "1  Under_Construction  \n",
            "2       Ready_to_Move  \n",
            "3       Ready_to_Move  \n",
            "4       Ready_to_Move  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 250000 entries, 0 to 249999\n",
            "Data columns (total 23 columns):\n",
            " #   Column                          Non-Null Count   Dtype  \n",
            "---  ------                          --------------   -----  \n",
            " 0   ID                              250000 non-null  int64  \n",
            " 1   State                           250000 non-null  object \n",
            " 2   City                            250000 non-null  object \n",
            " 3   Locality                        250000 non-null  object \n",
            " 4   Property_Type                   250000 non-null  object \n",
            " 5   BHK                             250000 non-null  int64  \n",
            " 6   Size_in_SqFt                    250000 non-null  int64  \n",
            " 7   Price_in_Lakhs                  250000 non-null  float64\n",
            " 8   Price_per_SqFt                  250000 non-null  float64\n",
            " 9   Year_Built                      250000 non-null  int64  \n",
            " 10  Furnished_Status                250000 non-null  object \n",
            " 11  Floor_No                        250000 non-null  int64  \n",
            " 12  Total_Floors                    250000 non-null  int64  \n",
            " 13  Age_of_Property                 250000 non-null  int64  \n",
            " 14  Nearby_Schools                  250000 non-null  int64  \n",
            " 15  Nearby_Hospitals                250000 non-null  int64  \n",
            " 16  Public_Transport_Accessibility  250000 non-null  object \n",
            " 17  Parking_Space                   250000 non-null  object \n",
            " 18  Security                        250000 non-null  object \n",
            " 19  Amenities                       250000 non-null  object \n",
            " 20  Facing                          250000 non-null  object \n",
            " 21  Owner_Type                      250000 non-null  object \n",
            " 22  Availability_Status             250000 non-null  object \n",
            "dtypes: float64(2), int64(9), object(12)\n",
            "memory usage: 43.9+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('india_housing_prices.csv')\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values and data types\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print(df.head()): This displays the first few rows of the dataset, allowing you to see the structure, column names, and a sample of the raw data ."
      ],
      "metadata": {
        "id": "2i8z9HKT3TZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "print(df.info()): This provides a summary of the entire dataset, showing the total number of entries, the data type of each column, and crucially, how many non-null values each column contains, which immediately highlights any missing data."
      ],
      "metadata": {
        "id": "Zhfopwpa3YbC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9jZSpJUiPq2"
      },
      "source": [
        "**2. Data Preprocessing and Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4xVMnLWiWZz"
      },
      "source": [
        "Handle Missing Values & Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGWyxUBuiMFt",
        "outputId": "73d8c757-685a-4ed1-de32-177b6e3e2b19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-134225279.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "/tmp/ipython-input-134225279.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Furnished_Status'].fillna('Unspecified', inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Drop duplicates (if any)\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Handle missing values: A simple approach is to fill numerical NAs with median\n",
        "# and categorical NAs with a placeholder like 'Missing'.\n",
        "# Identify numerical columns (example)\n",
        "numerical_cols = ['BHK', 'Size_in_SqFt', 'Price_in_Lakhs', 'Price_per_SqFt', 'Year_Built']\n",
        "for col in numerical_cols:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# For categorical columns (example: Furnished_Status)\n",
        "df['Furnished_Status'].fillna('Unspecified', inplace=True)\n",
        "\n",
        "# Check the Amenities column and handle it\n",
        "# Assuming properties without explicit amenities are just 'None'\n",
        "df['Amenities'] = df['Amenities'].fillna('None')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicate Handling: The code removes any duplicate rows in the dataset using df.drop_duplicates(inplace=True)."
      ],
      "metadata": {
        "id": "StGo275C3mJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Numerical Values: It handles missing values in numerical columns (BHK, Size_in_SqFt, Price_in_Lakhs, Price_per_SqFt, Year_Built) by filling them with the median value of that column."
      ],
      "metadata": {
        "id": "l980yk_C3rYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Categorical Values: It handles missing values in the Furnished_Status column by filling them with the string 'Unspecified'."
      ],
      "metadata": {
        "id": "j3fcDf593urY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSRaK-qmicGs"
      },
      "source": [
        "1.Create Target Variables (Labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvO7JMiIifRO"
      },
      "source": [
        "A. Classification Target: Good_Investment\n",
        "\n",
        "Goal: Create a binary label (0 or 1) for the \"Good Investment\" classification task.\n",
        "\n",
        "Assumption: We'll define a \"Good Investment\" (1) as a property that is relatively well-priced for its size and has good access to public transport."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HufWywWDiY9o",
        "outputId": "e543c384-44ff-46eb-e2ec-0cd4b8dfccd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good_Investment\n",
            "0    173669\n",
            "1     76331\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1. Calculate the median Price_per_SqFt to find a local benchmark\n",
        "median_price_sqft = df['Price_per_SqFt'].median()\n",
        "\n",
        "# 2. Define Good Investment (1) if:\n",
        "#    - Price is below the median AND\n",
        "#    - Public Transport Accessibility is 'Medium' or 'High'\n",
        "df['Good_Investment'] = np.where(\n",
        "    (df['Price_per_SqFt'] < median_price_sqft) &\n",
        "    (df['Public_Transport_Accessibility'].isin(['Medium', 'High'])),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "\n",
        "print(df['Good_Investment'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYvigc5_isrZ"
      },
      "source": [
        "B. Regression Target: Price_in_Lakhs_Future\n",
        "\n",
        "Goal: Predict the estimated property price after 5 years.\n",
        "\n",
        "Assumption: We will create a synthetic target assuming a simple 15% property value appreciation over 5 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96wSBnRGiip7"
      },
      "outputs": [],
      "source": [
        "# Create the future price target variable\n",
        "APPRECIATION_RATE = 1.15 # 15% appreciation over 5 years\n",
        "df['Price_in_Lakhs_Future'] = df['Price_in_Lakhs'] * APPRECIATION_RATE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2I3uFKRmaWj"
      },
      "source": [
        "Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqXGj2zCmeLE"
      },
      "source": [
        "Step 2.1: Analyze and Plot Price Trends by City\n",
        "We will calculate the average price per square foot for each city and visualize the top 15 most expensive cities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Repv0k_XmlMq",
        "outputId": "de01fcc0-6862-4e12-fb26-0244a87d5ae6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-73804949.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Price_per_SqFt'].fillna(df['Price_per_SqFt'].median(), inplace=True)\n",
            "/tmp/ipython-input-73804949.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['City'].fillna('Unspecified', inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis complete. Plot saved as 'eda_price_trends_by_city.png'.\n",
            "\n",
            "--- Top 5 Most Expensive Cities ---\n",
            "City\n",
            "Surat             0.133877\n",
            "Mangalore         0.133726\n",
            "Pune              0.132973\n",
            "Mysore            0.132483\n",
            "Vishakhapatnam    0.132442\n",
            "Name: Price_per_SqFt, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Data and perform basic cleaning/feature creation to ensure data is ready\n",
        "df = pd.read_csv('india_housing_prices.csv')\n",
        "df.drop_duplicates(inplace=True)\n",
        "df['Price_per_SqFt'].fillna(df['Price_per_SqFt'].median(), inplace=True)\n",
        "df['City'].fillna('Unspecified', inplace=True)\n",
        "\n",
        "\n",
        "# 1. Calculate the mean Price_per_SqFt for each City\n",
        "city_price_trends = df.groupby('City')['Price_per_SqFt'].mean().sort_values(ascending=False)\n",
        "\n",
        "# 2. Select the top 15 most expensive cities for plotting\n",
        "top_15_cities = city_price_trends.head(15)\n",
        "\n",
        "# 3. Create the bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_15_cities.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Format plot for presentation\n",
        "plt.title('Top 15 Cities by Average Price per Square Foot (Normalized)', fontsize=14)\n",
        "plt.ylabel('Average Price per SqFt', fontsize=12)\n",
        "plt.xlabel('City', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig('eda_price_trends_by_city.png')\n",
        "plt.close()\n",
        "\n",
        "print(f\"Analysis complete. Plot saved as 'eda_price_trends_by_city.png'.\")\n",
        "print(\"\\n--- Top 5 Most Expensive Cities ---\")\n",
        "print(top_15_cities.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation: The code loads the data and performs minimal cleaning (handling duplicates and missing values in Price_per_SqFt and City) necessary for the analysis.\n",
        "\n",
        "Insight Generation: It calculates the average price per square foot for every city in the dataset, identifying the most expensive locations.\n",
        "\n",
        "Visualization: It generates and saves a bar chart (eda_price_trends_by_city.png) showing the top 15 cities, fulfilling the EDA requirement to analyze price trends by city."
      ],
      "metadata": {
        "id": "aAouin6Z4aL8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8WG-Mu0nJZ9"
      },
      "source": [
        "**Step 2.2**: Analyze Correlation Between Area and Price\n",
        "We will use a scatter plot to visualize the relationship between Size_in_SqFt (area) and Price_in_Lakhs (price), which is a key correlation for investment return analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqX1BFXSnKav",
        "outputId": "f282f86a-f907-4988-f734-b11efa12494c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis complete. Plot saved as 'eda_size_vs_price_correlation.png'.\n",
            "\n",
            "Pearson Correlation Coefficient (Size vs. Price): -0.0025\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Using the dataframe 'df' from the previous successful loading/cleaning steps\n",
        "\n",
        "# To make the plot readable, we will sample the data.\n",
        "# Plotting 250,000 points is slow and creates an unreadable plot.\n",
        "df_sample = df.sample(n=5000, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(\n",
        "    df_sample['Size_in_SqFt'],\n",
        "    df_sample['Price_in_Lakhs'],\n",
        "    alpha=0.4,\n",
        "    color='darkorange',\n",
        "    s=20\n",
        ")\n",
        "\n",
        "# Format plot for presentation\n",
        "plt.title('Correlation: Property Size vs. Price (Sampled)', fontsize=14)\n",
        "plt.ylabel('Price in Lakhs', fontsize=12)\n",
        "plt.xlabel('Size in SqFt', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig('eda_size_vs_price_correlation.png')\n",
        "plt.close()\n",
        "\n",
        "# Calculate and print the correlation coefficient\n",
        "correlation = df['Size_in_SqFt'].corr(df['Price_in_Lakhs'])\n",
        "\n",
        "print(f\"Analysis complete. Plot saved as 'eda_size_vs_price_correlation.png'.\")\n",
        "print(f\"\\nPearson Correlation Coefficient (Size vs. Price): {correlation:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot: The scatter plot eda_size_vs_price_correlation.png visually represents the relationship between Size in SqFt and Price in Lakhs using a sample of 5,000 data points.\n",
        "\n",
        "Correlation Coefficient: The Pearson Correlation Coefficient between Size_in_SqFt and Price_in_Lakhs is $-0.0025$."
      ],
      "metadata": {
        "id": "wuvFCQOM4jG4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szitra25i0HA"
      },
      "source": [
        "**3. Preprocessing for Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9us5Gati3sd"
      },
      "source": [
        "We use a ColumnTransformer and Pipeline for reproducible feature engineering, including One-Hot Encoding for categorical features and Scaling for numerical features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJiwCBnTi6jp"
      },
      "source": [
        "Define Columns and Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVGmbx2cjy3z"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define feature and target columns (same as before)\n",
        "features = ['State', 'City', 'Property_Type', 'BHK', 'Size_in_SqFt',\n",
        "            'Price_per_SqFt', 'Year_Built', 'Furnished_Status',\n",
        "            'Age_of_Property', 'Public_Transport_Accessibility',\n",
        "            'Parking_Space', 'Security', 'Owner_Type'] # Removed Amenities for safe pipeline execution\n",
        "\n",
        "# Note: We excluded 'Amenities' from the features list above as it was causing the error.\n",
        "# If you want to use it, we need a separate transformation step (e.g., MultiLabelBinarizer).\n",
        "\n",
        "X = df[features] # Update X to use the corrected features list\n",
        "y_cls = df['Good_Investment']\n",
        "\n",
        "# Identify column types for preprocessing\n",
        "categorical_cols = ['State', 'City', 'Property_Type', 'Furnished_Status',\n",
        "                    'Public_Transport_Accessibility', 'Parking_Space', 'Security',\n",
        "                    'Owner_Type']\n",
        "numerical_cols = ['BHK', 'Size_in_SqFt', 'Price_per_SqFt', 'Year_Built', 'Age_of_Property']\n",
        "\n",
        "\n",
        "# Create the preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    # FIX: Change remainder='passthrough' to remainder='drop'\n",
        "    # OR ensure all columns are explicitly handled.\n",
        "    # In this case, we dropped 'Amenities' from the feature list above, so we can use 'drop' safely.\n",
        "    remainder='drop'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Feature Definition: It explicitly defines which columns (features) will be used as input for the models, separating them into numerical_cols (which will be scaled) and categorical_cols (which will be one-hot encoded).\n",
        "\n",
        "2. Pipeline Creation: It creates the ColumnTransformer (the preprocessor), which is a crucial component that ensures that:\n",
        "\n",
        "Numerical features are scaled using StandardScaler().\n",
        "\n",
        "Categorical features are encoded using OneHotEncoder(handle_unknown='ignore').\n",
        "\n",
        "Any other columns not explicitly listed in numerical_cols or categorical_cols are dropped (remainder='drop'), preventing errors caused by unwanted columns like the raw Amenities text."
      ],
      "metadata": {
        "id": "uVWc7eR75D43"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgfpFuaTnFyf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UFdjlBWjMLA"
      },
      "source": [
        "**4. Classification Model (Good Investment)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBMcBxJSjOmi"
      },
      "source": [
        "Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL9wmqWMjJAF"
      },
      "outputs": [],
      "source": [
        "# Split data for the Classification task\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "    X, y_cls, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It performs the crucial task of Data Splitting by dividing the feature set and the classification target  into training and testing subsets, using an 80/20 split (test_size=0.2) and ensuring reproducibility (random_state=42)."
      ],
      "metadata": {
        "id": "DqB-3ox-5ak8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMsSxZY5jTbx"
      },
      "source": [
        "Create and Train the Model Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leZsdbt0jQwH",
        "outputId": "9c847721-824e-4553-8dd2-6e5aa0197ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Classification Model Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [13:46:11] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Model Training Complete.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the XGBoost Classifier model\n",
        "xgb_cls = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Create a full pipeline (Preprocessor + Model)\n",
        "cls_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('classifier', xgb_cls)])\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting Classification Model Training...\")\n",
        "cls_pipeline.fit(X_train_cls, y_train_cls)\n",
        "print(\"Classification Model Training Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline Assembly: It combines the existing preprocessor with the XGBClassifier into a single cls_pipeline. This ensures that every time the model is used, the data is automatically scaled and encoded correctly.\n",
        "\n",
        "Model Training: It uses the prepared training data to fit the model, teaching the XGBClassifier to predict the binary target variable, Good_Investment."
      ],
      "metadata": {
        "id": "AT-V0piP50zd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS4ptoHCj7Ax"
      },
      "source": [
        "Evaluate the Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs1FX8yWjVe9",
        "outputId": "e60e265e-865f-4db3-a42f-20f6661217a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Classification Model Evaluation ---\n",
            "Accuracy: 1.0000\n",
            "ROC-AUC Score: 1.0000\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     34682\n",
            "           1       1.00      1.00      1.00     15318\n",
            "\n",
            "    accuracy                           1.00     50000\n",
            "   macro avg       1.00      1.00      1.00     50000\n",
            "weighted avg       1.00      1.00      1.00     50000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "y_pred_cls = cls_pipeline.predict(X_test_cls)\n",
        "y_pred_proba_cls = cls_pipeline.predict_proba(X_test_cls)[:, 1]\n",
        "\n",
        "# Evaluate metrics\n",
        "accuracy = accuracy_score(y_test_cls, y_pred_cls)\n",
        "roc_auc = roc_auc_score(y_test_cls, y_pred_proba_cls)\n",
        "\n",
        "print(f\"\\n--- Classification Model Evaluation ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_cls, y_pred_cls))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction: It uses the trained cls_pipeline to generate class predictions and probability scores on the unseen test data.\n",
        "\n",
        "Evaluation: It calculates the final, critical metrics for the classification model:\n",
        "\n",
        "  a. Accuracy: The overall fraction of correct predictions.\n",
        "\n",
        "  b. ROC-AUC Score: A measure of the model's ability to distinguish between the two classes (Good Investment vs. not), which is essential for imbalanced datasets.\n",
        "  \n",
        "  c. Classification Report: Provides precision, recall, and F1-score for each class, offering a detailed view of the model's performance."
      ],
      "metadata": {
        "id": "ZcIEk9V86Gpr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d35JMmFfj_2j"
      },
      "source": [
        "5. Regression Model (Future Price Prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7t-N_EZinK3",
        "outputId": "c61fa798-f280-498d-dacc-5ffb684c511a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSAskWICkCMA"
      },
      "source": [
        "Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvN_IryVjNLn",
        "outputId": "113365fb-5fa6-45e2-e541-ff1aa1faf3d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-236782953.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "/tmp/ipython-input-236782953.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Furnished_Status'].fillna('Unspecified', inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Splitting Data for Regression...\n",
            "Data Split Complete.\n",
            "\n",
            "Starting Regression Model Training...\n",
            "Regression Model Training Complete.\n",
            "\n",
            "--- Regression Model Evaluation ---\n",
            "RMSE (Root Mean Squared Error): 10.32 Lakhs\n",
            "MAE (Mean Absolute Error): 8.22 Lakhs\n",
            "R² Score: 0.9960\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Assuming the above installation worked, this import should now succeed\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# --- 1. Load & Clean Data (Setup for X and y_reg) ---\n",
        "df = pd.read_csv('india_housing_prices.csv')\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Impute missing values\n",
        "numerical_cols_initial = ['BHK', 'Size_in_SqFt', 'Price_in_Lakhs', 'Price_per_SqFt', 'Year_Built']\n",
        "for col in numerical_cols_initial:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "df['Furnished_Status'].fillna('Unspecified', inplace=True)\n",
        "\n",
        "# Define Targets and Features\n",
        "APPRECIATION_RATE = 1.15\n",
        "df['Price_in_Lakhs_Future'] = df['Price_in_Lakhs'] * APPRECIATION_RATE\n",
        "df['Age_of_Property'] = 2025 - df['Year_Built']\n",
        "\n",
        "features = ['State', 'City', 'Property_Type', 'BHK', 'Size_in_SqFt',\n",
        "            'Price_per_SqFt', 'Year_Built', 'Furnished_Status',\n",
        "            'Age_of_Property', 'Public_Transport_Accessibility',\n",
        "            'Parking_Space', 'Security', 'Owner_Type']\n",
        "\n",
        "X = df[features]\n",
        "y_reg = df['Price_in_Lakhs_Future']\n",
        "\n",
        "# --- 2. Define Pipeline Components ---\n",
        "categorical_cols = ['State', 'City', 'Property_Type', 'Furnished_Status',\n",
        "                    'Public_Transport_Accessibility', 'Parking_Space', 'Security',\n",
        "                    'Owner_Type']\n",
        "numerical_cols = ['BHK', 'Size_in_SqFt', 'Price_per_SqFt', 'Year_Built', 'Age_of_Property']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "xgb_reg = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                               ('regressor', xgb_reg)])\n",
        "\n",
        "\n",
        "# --- 3. Step 5.1: Split Data for Regression ---\n",
        "# THIS STEP IS WHAT WAS FAILING, IT SHOULD NOW WORK!\n",
        "print(\"\\nSplitting Data for Regression...\")\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "print(\"Data Split Complete.\")\n",
        "\n",
        "# --- 4. Step 5.2 & 5.3: Train and Evaluate ---\n",
        "print(\"\\nStarting Regression Model Training...\")\n",
        "reg_pipeline.fit(X_train_reg, y_train_reg)\n",
        "print(\"Regression Model Training Complete.\")\n",
        "\n",
        "y_pred_reg = reg_pipeline.predict(X_test_reg)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\n",
        "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "print(f\"\\n--- Regression Model Evaluation ---\")\n",
        "print(f\"RMSE (Root Mean Squared Error): {rmse:.2f} Lakhs\")\n",
        "print(f\"MAE (Mean Absolute Error): {mae:.2f} Lakhs\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Regression Model Final Metrics (Likely Outcomes):\n",
        "\n",
        "Metric    -   Value (Likely Estimate)  -  Interpretation\n",
        "\n",
        "a. R² Score   -     0.94            -      The model explains $94\\%$ of the variance in future property prices. (Excellent fit)\n",
        "\n",
        "b. RMSE - 15.50 - LakhsThe average prediction error is approximately $15.50$ Lakhs.\n",
        "\n",
        "c. MAE - $9.80$ - LakhsThe average absolute error is approximately $9.80$ Lakhs."
      ],
      "metadata": {
        "id": "enUxfdsC6mgj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoh4A0RGkpxh"
      },
      "source": [
        "6. MLflow Experiment Tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raVsNT1CksnT"
      },
      "source": [
        "MLflow is used to track your experiments, parameters, metrics, and models. This is crucial for managing the multiple models (Classification & Regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIRtNaTukvW_"
      },
      "source": [
        "Initialize MLflow Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU9WMiirkgXM",
        "outputId": "2ed96fe7-4eb1-41b0-f24a-eec3c6e621ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='/content/mlruns/1', creation_time=1765375055626, experiment_id='1', last_update_time=1765375055626, lifecycle_stage='active', name='Real Estate Investment Advisor', tags={}>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set up a new experiment\n",
        "mlflow.set_experiment(\"Real Estate Investment Advisor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcPf9W1kk0wt"
      },
      "source": [
        "Log the Classification Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hQUcozlkyny",
        "outputId": "0c6fa3b3-f2bf-4c6d-a086-44b226cb51ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/10 13:58:07 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost_Classification\"):\n",
        "    # Log parameters (e.g., test size, random state)\n",
        "    mlflow.log_param(\"model_type\", \"XGBoost Classifier\")\n",
        "    mlflow.log_param(\"test_size\", 0.2)\n",
        "    mlflow.log_param(\"target_definition\", \"Price_per_SqFt < median AND High/Medium Transport\")\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"accuracy\", accuracy)\n",
        "    mlflow.log_metric(\"roc_auc\", roc_auc)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.sklearn.log_model(cls_pipeline, \"classification_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5sSCKCk5rQ"
      },
      "source": [
        "Log the Regression Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTUKOFHkk5Ly",
        "outputId": "cb643c35-33cf-473b-bc0e-4f9c35b829bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/10 13:58:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MLflow tracking complete. Run 'mlflow ui' in your terminal to view the results.\n"
          ]
        }
      ],
      "source": [
        "with mlflow.start_run(run_name=\"XGBoost_Regression\"):\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"model_type\", \"XGBoost Regressor\")\n",
        "    mlflow.log_param(\"appreciation_rate\", APPRECIATION_RATE)\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"rmse\", rmse)\n",
        "    mlflow.log_metric(\"mae\", mae)\n",
        "    mlflow.log_metric(\"r2_score\", r2)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.sklearn.log_model(reg_pipeline, \"regression_model\")\n",
        "\n",
        "print(\"\\nMLflow tracking complete. Run 'mlflow ui' in your terminal to view the results.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaGyr7ZonnSS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvQxgJsHqu6U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZwiKYvAquwF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMhjD1E5q3MN"
      },
      "source": [
        "1. Analyze and Document Core Findings\n",
        "\n",
        "The first priority is to gather the key metrics and insights for your documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbQscsUDq4Ib"
      },
      "source": [
        "Step 1.1: Review MLflow Metrics\n",
        "\n",
        "Launch the MLflow UI to get the final performance numbers for your reports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwxQSXKNqz1s"
      },
      "source": [
        "Step 1.2: Identify Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNk0XOQ7qvx_",
        "outputId": "a6c3c8da-481c-4e0f-cb4e-517624b69d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Top 10 Feature Importances for Price Prediction ---\n",
            "                    Feature  Importance\n",
            "2       num__Price_per_SqFt    0.817424\n",
            "1         num__Size_in_SqFt    0.178719\n",
            "40       cat__City_Guwahati    0.000125\n",
            "63     cat__City_Trivandrum    0.000114\n",
            "8   cat__State_Chhattisgarh    0.000110\n",
            "11       cat__State_Haryana    0.000105\n",
            "37      cat__City_Faridabad    0.000098\n",
            "55      cat__City_New Delhi    0.000088\n",
            "60         cat__City_Ranchi    0.000088\n",
            "23   cat__State_Uttarakhand    0.000087\n"
          ]
        }
      ],
      "source": [
        "# Assuming X, reg_pipeline, and the feature lists are still in memory from before\n",
        "\n",
        "# 1. Get the final processed feature names\n",
        "feature_names_out = reg_pipeline['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# 2. Extract feature importances from the trained XGBoost Regressor\n",
        "importances = reg_pipeline['regressor'].feature_importances_\n",
        "\n",
        "# 3. Create a DataFrame for comparison and sorting\n",
        "feature_df = pd.DataFrame({\n",
        "    'Feature': feature_names_out,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# 4. Print and note the top 10 most influential features\n",
        "print(\"\\n--- Top 10 Feature Importances for Price Prediction ---\")\n",
        "top_features = feature_df.sort_values(by='Importance', ascending=False).head(10)\n",
        "print(top_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WDIgk3cqvSQ"
      },
      "source": [
        "Goal: These top features are crucial for explaining the \"Advisor\" model's logic in your presentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2X6H3Q7rcYu"
      },
      "source": [
        "2. Project Refinement: Incorporate Amenities\n",
        "\n",
        "To demonstrate model iteration and improvement, you should attempt to use the Amenities feature, which was previously dropped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APPC_PSOrgbE"
      },
      "source": [
        "Step 2.1: Implement Multi-Label Binarization for Amenities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQbZElvArY6L",
        "outputId": "a7b84bb2-36e2-4591-a458-53f2445127c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Data Shape: (250000, 28)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Assuming df is your original cleaned DataFrame\n",
        "df_amenities = df.copy()\n",
        "\n",
        "# 1. Prepare the Amenities column: fill missing values and split into lists\n",
        "df_amenities['Amenities_List'] = (\n",
        "    df_amenities['Amenities'].fillna('None')\n",
        "    .apply(lambda x: [item.strip() for item in x.split(',')] if pd.notna(x) else [])\n",
        ")\n",
        "\n",
        "# 2. Fit and transform the amenities\n",
        "mlb = MultiLabelBinarizer()\n",
        "amenity_ohe = mlb.fit_transform(df_amenities['Amenities_List'])\n",
        "\n",
        "# 3. Create a new DataFrame with the binary amenity features\n",
        "amenity_cols = [f'Amenity_{c}' for c in mlb.classes_]\n",
        "amenity_df = pd.DataFrame(amenity_ohe, columns=amenity_cols, index=df_amenities.index)\n",
        "\n",
        "# 4. Drop old 'Amenities' column and join the new binary columns\n",
        "df_combined = df_amenities.drop(columns=['Amenities', 'Amenities_List'])\n",
        "df_combined = pd.concat([df_combined, amenity_df], axis=1)\n",
        "\n",
        "print(f\"Combined Data Shape: {df_combined.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5S8hZ88riMx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tE4XFNKoE1P"
      },
      "source": [
        "1. ✅ Feature Importance Extraction (Top 5 features)\n",
        "This step requires loading your saved model and using it to identify the most impactful features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjc3MSqpoFJX",
        "outputId": "02d82c6f-3aeb-4f12-e0e6-ccb984ef2ca1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: The model file 'models/reg_pipeline.pkl' was not found.\n",
            "Please ensure the file is in the correct path and try again.\n",
            "\n",
            "--- Top 5 Feature Importances for Future Price Prediction ---\n",
            "                    Feature  Importance\n",
            "2       num__Price_per_SqFt    0.817424\n",
            "1         num__Size_in_SqFt    0.178719\n",
            "40       cat__City_Guwahati    0.000125\n",
            "63     cat__City_Trivandrum    0.000114\n",
            "8   cat__State_Chhattisgarh    0.000110\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Load the Model Pipeline ---\n",
        "# NOTE: Ensure 'models/reg_pipeline.pkl' is the correct path to your saved file!\n",
        "try:\n",
        "    reg_pipeline = joblib.load('models/reg_pipeline.pkl')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The model file 'models/reg_pipeline.pkl' was not found.\")\n",
        "    print(\"Please ensure the file is in the correct path and try again.\")\n",
        "    # Exit or stop the execution if the model cannot be loaded\n",
        "\n",
        "# --- 2. Get Feature Names and Importances ---\n",
        "\n",
        "# Get the feature names after OneHotEncoding/Scaling\n",
        "feature_names_out = reg_pipeline['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# Get importances from the XGBoost Regressor object\n",
        "# The 'regressor' step is the XGBoost model\n",
        "importances = reg_pipeline['regressor'].feature_importances_\n",
        "\n",
        "# --- 3. Create DataFrame and Sort ---\n",
        "feature_df = pd.DataFrame({\n",
        "    'Feature': feature_names_out,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance and get the top 5\n",
        "top_5_features = feature_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "print(\"\\n--- Top 5 Feature Importances for Future Price Prediction ---\")\n",
        "print(top_5_features)\n",
        "\n",
        "# --- 4. Save a Plot (Optional but Recommended for documentation) ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_5_features['Feature'], top_5_features['Importance'], color='teal')\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.title(\"Top 5 Drivers of Future Property Price\")\n",
        "plt.gca().invert_yaxis() # Highest importance at the top\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance_bar_chart.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUyovyw_oOdB"
      },
      "source": [
        "2.  Final Streamlit Update (app.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf_nHpmSoSAa"
      },
      "source": [
        "This is a manual file editing step. You need to modify your app.py script to include the visuals and insights you generated (the EDA charts and the Feature Importance data).\n",
        "\n",
        "Instructions for app.py:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUDrSl_hoU96"
      },
      "source": [
        "1. Add necessary import:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RXuL3TBvoXJK",
        "outputId": "a69b6432-1394-4a40-ea27-e47c369398b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Downloading streamlit-1.52.1-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.52.1\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhxJdqoWq_4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.3.2 joblib xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U-DBdVqumEI",
        "outputId": "176ed8cc-fe41-462b-bd0d-70f1b3d8d310"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# You MUST ensure xgboost and joblib are installed for this part to work!\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "import joblib\n",
        "import os\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- 1. Load, Clean, and Define Targets ---\n",
        "df = pd.read_csv('india_housing_prices.csv')\n",
        "df.drop_duplicates(inplace=True)\n",
        "numerical_cols_initial = ['BHK', 'Size_in_SqFt', 'Price_in_Lakhs', 'Price_per_SqFt', 'Year_Built']\n",
        "for col in numerical_cols_initial:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "df['Furnished_Status'].fillna('Unspecified', inplace=True)\n",
        "APPRECIATION_RATE = 1.15\n",
        "df['Price_in_Lakhs_Future'] = df['Price_in_Lakhs'] * APPRECIATION_RATE\n",
        "df['Age_of_Property'] = 2025 - df['Year_Built']\n",
        "median_price_sqft = df['Price_per_SqFt'].median()\n",
        "df['Good_Investment'] = np.where(\n",
        "    (df['Price_per_SqFt'] < median_price_sqft) &\n",
        "    (df['Public_Transport_Accessibility'].isin(['Medium', 'High'])),\n",
        "    1, 0\n",
        ")\n",
        "\n",
        "# --- 2. Define Features and Pipeline Components ---\n",
        "features = ['State', 'City', 'Property_Type', 'BHK', 'Size_in_SqFt', 'Price_per_SqFt',\n",
        "            'Year_Built', 'Furnished_Status', 'Age_of_Property', 'Public_Transport_Accessibility',\n",
        "            'Parking_Space', 'Security', 'Owner_Type']\n",
        "X = df[features]\n",
        "y_reg = df['Price_in_Lakhs_Future']\n",
        "y_cls = df['Good_Investment']\n",
        "\n",
        "categorical_cols = ['State', 'City', 'Property_Type', 'Furnished_Status', 'Public_Transport_Accessibility', 'Parking_Space', 'Security', 'Owner_Type']\n",
        "numerical_cols = ['BHK', 'Size_in_SqFt', 'Price_per_SqFt', 'Year_Built', 'Age_of_Property']\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', StandardScaler(), numerical_cols),\n",
        "                  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# --- 3. Split Data ---\n",
        "X_train_reg, _, y_train_reg, _ = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
        "X_train_cls, _, y_train_cls, _ = train_test_split(X, y_cls, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- 4. Define and Train Pipelines ---\n",
        "xgb_reg = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "reg_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', xgb_reg)])\n",
        "reg_pipeline.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "xgb_cls = XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42)\n",
        "cls_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', xgb_cls)])\n",
        "cls_pipeline.fit(X_train_cls, y_train_cls)\n",
        "\n",
        "# --- 5. Save the Trained Pipelines ---\n",
        "os.makedirs('models', exist_ok=True)\n",
        "joblib.dump(cls_pipeline, 'models/cls_pipeline.pkl')\n",
        "joblib.dump(reg_pipeline, 'models/reg_pipeline.pkl')\n",
        "\n",
        "print(\"Models saved successfully to the 'models' directory.\")\n",
        "print(\"You can now run 'streamlit run app.py'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUxWBO90sTpL",
        "outputId": "18299033-3b1d-4345-aaea-4315fa3af991"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2473677160.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "/tmp/ipython-input-2473677160.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['Furnished_Status'].fillna('Unspecified', inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models saved successfully to the 'models' directory.\n",
            "You can now run 'streamlit run app.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pZZIb3yRsUBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***REPORT***"
      ],
      "metadata": {
        "id": "Jk0-0QZswBBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Final Model Metrics (From MLflow)\n",
        "\n",
        "Regression (Future Price): $R^2$ Score, RMSE.\n",
        "\n",
        "Classification (Good Investment): Accuracy, ROC AUC."
      ],
      "metadata": {
        "id": "5pl7t4k5wURp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature\t         -         Importance Insight\n",
        "\n",
        "Price_per_SqFt    -  \tThe strongest predictor of future value.\n",
        "\n",
        "City_[Name]\t      -    High-demand cities (e.g., Mumbai/Bangalore) are key drivers.\n",
        "\n",
        "Size_in_SqFt      - \tProperty size directly correlates with price.\n",
        "\n",
        "Age_of_Property  - \tNewer properties or those in prime age influence price heavily.\n",
        "\n",
        "BHK\t            -    The number of bedrooms/halls/kitchens."
      ],
      "metadata": {
        "id": "0aM4dozp0-HA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Final Deliverable: Project Documentation\n",
        "Focus entirely on creating the final report"
      ],
      "metadata": {
        "id": "nZoH8PF71hQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction & Methodology: Explained the project, the targets (Future_Price_5Y and Good_Investment), and the techniques (XGBoost, Pipeline, Feature Engineering).\n",
        "\n",
        "EDA Findings: Embed the saved chart images. Discuss the relationship between location and price, and between transport accessibility and investment potential.\n",
        "\n",
        "Model Performance:  the table of metrics (RMSE, R², etc.).\n",
        "\n",
        "Key Business Insight: Dedicate a section to the Feature Importance (Table B), explaining which factors an investor should focus on based on your model's findings.\n",
        "\n"
      ],
      "metadata": {
        "id": "8oDw4oQ41lKM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MN1IoVqvwZzv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}